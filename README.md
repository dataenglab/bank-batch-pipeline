# Batch Processing Data Architecture Portfolio

## Overview
This is a batch-processing-based data architecture developed as a university portfolio project. The system ingests, processes, and aggregates large volumes of bank transaction data for quarterly machine learning model updates. The project features a microservices architecture with comprehensive data validation, error handling, and monitoring capabilities.

## Features
- **Batch Data Ingestion**: Monthly ingestion of transaction data with validation
- **Data Processing**: Quarterly batch processing with aggregation and feature engineering
- **Data Quality Framework**: Multi-layer validation for data integrity
- **Advanced Error Handling**: Automatic retry with exponential backoff
- **Real-time Monitoring**: Custom metrics server for performance tracking
- **Infrastructure as Code**: Reproducible Docker-based deployment
- **Microservices Architecture**: Isolated, scalable components
- **Data Security**: Encryption, access control, and audit trails

## System Architecture

### Data Pipeline Flow

Data Sources → [Ingestion Service] → [Raw Storage] → [Batch Processor] → [Database] → [API] → ML Application
↓ ↓ ↓ ↓ ↓
[Validation] [Error Handler] [Monitoring] [Backup] [Authentication]


### Component Description
- **Data Sources**: CSV/JSON files with bank transaction data
- **Ingestion Service**: Monthly batch data loading with validation
- **Raw Storage**: MinIO object storage for raw data backup
- **Batch Processor**: Quarterly processing with aggregation
- **Database**: PostgreSQL for processed and structured data
- **API**: REST endpoints for ML application consumption
- **ML Application**: Quarterly model training and updates

### Supporting Services
- **Validation**: Multi-layer data quality checks and business rules
- **Error Handler**: Automatic retry mechanisms with exponential backoff
- **Monitoring**: Real-time performance metrics and tracking
- **Backup**: Automated data protection and disaster recovery
- **Authentication**: Service access control and security

## Functional Requirements

### Data Ingestion and Storage
- System ingests CSV/JSON transaction data in monthly batches
- Raw data stored in MinIO object storage for audit and backup
- Processed data stored in PostgreSQL with ACID compliance
- Supports datasets with 1,000,000+ records with timestamps

### Data Processing and Validation
- Quarterly batch processing of accumulated transaction data
- Multi-layer data validation (business rules, data types, value ranges)
- Automated data cleansing and transformation
- Aggregation and feature engineering for ML consumption

### Error Handling and Reliability
- Comprehensive error classification and recovery strategies
- Automatic retry mechanisms with exponential backoff
- Transaction integrity maintenance throughout processing
- Detailed logging and error reporting

## Setup Instructions

### Prerequisites
- Docker and Docker Compose
- Python 3.9+
- Git

### Quick Start
1. **Clone the repository**
   ```bash
   git clone [your-repository-url]
   cd bank-batch-pipeline

2. Start the complete system
   docker-compose up -d
   
3. Run individual demonstrations
   
# Test data validation framework
python test_validator.py

# Test error handling system
python test_error_handler.py

# Run full pipeline demonstration
python src/advanced_pipeline.py

4. View Generated Reports
After running demonstrations, check the generated reports:

# View performance metrics
cat pipeline_performance_report.json

# Analyze error handling performance  
cat demo_error_analysis.json

# Check data validation results
cat demo_validation_errors.json

Performance Results
Processing Metrics
Success Rate: 80% (24/30 sample transactions processed successfully)

Error Handling: 100% effectiveness (31 errors handled with automatic recovery)

Processing Time: 32.72 seconds for sample batch of 30 transactions

Throughput: 0.73 records/second

Pipeline Efficiency: 80.0%

Error Handling Performance
Total Errors Handled: 31 across processing pipeline

Automatic Retries: 3 retry attempts with exponential backoff

Error Types: Data validation (6), Processing (24), Database connection (1)

Recovery Success: 100% of recoverable errors successfully handled

Technologies Used
Backend & Processing
Python 3.9: Data processing, validation, and business logic

PostgreSQL: ACID-compliant relational database

MinIO: S3-compatible object storage

Docker: Containerization and service isolation

Monitoring & Observability
Prometheus Client: Custom metrics collection

Structured Logging: Comprehensive operation tracking

Portfolio Documentation
Complete Documentation
portfolio/CONCEPTION_PHASE.md: Detailed architecture design and technology justification

portfolio/PORTFOLIO_SUMMARY.md: Comprehensive project documentation and results

Generated Evidence
Performance Reports: pipeline_performance_report.json

Error Analysis: demo_error_analysis.json

Validation Reports: demo_validation_errors.json

Execution Logs: advanced_pipeline_errors.log

License
This project is developed for educational purposes as part of the Data Engineering (DLMDSEDE02) course portfolio at IU International University of Applied Sciences
